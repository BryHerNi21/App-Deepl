import streamlit as st

st.set_page_config(
    page_title="Mundo Deep",
    page_icon="üåü",
    layout="wide"
)
# #Estilo de to
st.markdown("""
<style>

/* Primer contenedor */
.block-container {
    max-width: 1000px;
    padding-top: 1.5rem;
    padding-bottom: 3rem;
    margin: 0 auto;
}

/* Fondo blanco */
.stApp {
    background-color: #ffffff;
}

/* Hero */
.hero-box {
    background: radial-gradient(circle at top left, #e0f2ff, #f7fbff);
    border-radius: 24px;
    padding: 32px 30px;
    box-shadow: 0 10px 30px rgba(15, 23, 42, 0.08);
    margin-bottom: 26px;
}

.hero-title {
    font-size: 2.4rem;
    font-weight: 800;
    color: #102a43;
    margin-bottom: 0.3rem;
}

.hero-subtitle {
    font-size: 1.05rem;
    color: #334e68;
    max-width: 680px;
}

.tag-chip {
    display: inline-block;
    background-color: #dbeafe;
    color: #1d4ed8;
    padding: 4px 10px;
    border-radius: 999px;
    font-size: 0.8rem;
    font-weight: 600;
    letter-spacing: 0.03em;
    margin-bottom: 8px;
}

/* Tarjeta explicativa */
.card-explain {
    background: #ffffff;
    border-radius: 18px;
    border: 1px solid #e2e8f0;
    padding: 18px 20px;
    box-shadow: 0 6px 18px rgba(15, 23, 42, 0.04);
    margin-bottom: 24px;
    font-size: 0.98rem;
    color: #1f2933;
}

.section-title {
    font-size: 1.5rem;
    font-weight: 700;
    color: #1f3b5d;
    margin-top: 10px;
    margin-bottom: 6px;
}

.custom-list li {
    margin-bottom: 4px;
}

img {
    border-radius: 18px;
}
/*Las tarjetas de transformers*/
.topic-grid {
    display: flex;
    flex-wrap: wrap;
    justify-content: center;
    gap: 5rem;          /* <-- m√°s espacio horizontal y vertical */
    margin-top: 2.2rem;
    margin-bottom: 2.5rem;
}

/* Tarjeta base: fondo clarito, texto oscuro */
.topic-card {
    width: 260px;
    background: #f9fafb;                       /* fondo gris muy suave */
    border-radius: 18px;
    padding: 18px 18px 16px 18px;
    border: 1px solid #e5e7eb;
    box-shadow: 0 8px 18px rgba(15, 23, 42, 0.06);
    text-decoration: none !important;
    display: flex;
    flex-direction: column;
    gap: 8px;
    transition: transform 0.16s ease, box-shadow 0.16s ease, border-color 0.16s ease;
}

/* Variantes de color: borde lateral y fondo un poquito tintado */
.card-transformers {
    background: #e0f2fe;                       /* azul muy suave */
    border-left: 6px solid #3b82f6;
}
.card-callbacks {
    background: #ede9fe;                       /* lila muy suave */
    border-left: 6px solid #a855f7;
}
            
.card-regularizacion {
    background: #F5E6F5;                       /* lila muy suave */
    border-left: 6px solid #FF3DFF;
}

.card-LSTM {
    background: #ECFFEB;                       /* lila muy suave */
    border-left: 6px solid #4AFF3D;
}

.card-rna {
    background: #FFEBEB;                       /* lila muy suave */
    border-left: 6px solid #FF0000;
}
            
.card-perceptron {
    background: #FFFAF2;                       /* lila muy suave */
    border-left: 6px solid #FF8C00;
}
            
.card-embedding {
    background: #F0FFFF;                       /* lila muy suave */
    border-left: 6px solid #00FFFA;
}

/* Cabecera de la tarjeta */
.topic-header {
    display: flex;
    align-items: center;
    gap: 10px;
}

/* Icono redondo */
.topic-icon {
    width: 34px;
    height: 34px;
    border-radius: 999px;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 1.35rem;
    background: #ffffff55;
}

/* Texto: AHORA ES OSCURO, NO BLANCO */
.topic-title-text {
    font-weight: 700;
    color: #111827;
    font-size: 1.05rem;
}
.topic-desc {
    font-size: 0.9rem;
    color: #374151;
}

.topic-footer {
    font-size: 0.8rem;
    font-weight: 600;
    color: #1d4ed8;
}

/* Hover suave */
.topic-card:hover {
    transform: translateY(-6px);
    box-shadow: 0 14px 26px rgba(15, 23, 42, 0.14);
    border-color: #bfdbfe;
}

/* Quitar subrayado y azul horrible de link */
.topic-card:link,
.topic-card:visited,
.topic-card:hover,
.topic-card:active {
    text-decoration: none !important;
    color: inherit !important;
}

/* Bot√≥n volver */
.volver-btn button {
    border-radius: 999px !important;
}

</style>
""", unsafe_allow_html=True)




# ================== NAVEGACI√ìN ================== #
params = st.experimental_get_query_params()
page = params.get("page", ["home"])[0]


# ================== HOME ================== #
if page == "home":

    st.markdown("""
    <div class="hero-box">
        <div class="tag-chip">Curso de Deep Learning</div>
        <div class="hero-title">Bienvenidos al Mundo de Deep üåü</div>
        <div class="hero-subtitle">
                Hola! Ac√° vas a encontrar algunos temas de deep learning con explicaciones
                sencillas, ejemplos visuales y recursos para que no entres en p√°nico con este curso.
        </div>
    </div>
    """, unsafe_allow_html=True)
 
    st.markdown("""
    <div class="card-explain">

    <p><b>Pero.. tacho, primero que todo ¬øQu√© es el Deep Learning?</b></p>

    <p>
        Para "Barajarla" m√°s despacio y que no se agobien, el deep learning es un subcampo de
        la inteligencia artificial que intenta imitar el cerebro humano utilizando redes 
        neuronales con muchas capas para aprender patrones complejos a partir de los datos.
    </p>

    <p>
        Esta es la parte interesante, porque en vez de programar reglas, le mostramos ejemplos:
        Im√°genes, texto, audio entre otros, y el modelo aprende a reconocer, traducir, generar
        o predecir cosas por s√≠ solo.
    </p>

    <p>
        Pero esto no pasa por arte de magia (Ojal√°). Para lograr esto hay diferentes formas, 
        m√©todos y herramientas, y esta p√°gina est√° pensada precisamente para que no entren "crudos" a
        esta materia.‚ú®
    </p>

    </div>
    """, unsafe_allow_html=True)



    st.image(
        "assets/images/gatosentado.jpeg",
        caption="Sient√©nse y tomen nota, vamos a ser como este gato",
        use_container_width=True
    )
 
    st.markdown("""
<div class="card-explain">

<p><b>Ahora s√≠, hablemos de c√≥mo el Deep Learning intenta imitar nuestro cerebro</b></p>

<p>
    Ac√° viene la parte ch√©vere: las redes neuronales est√°n inspiradas en las neuronas humanas.
    No es que tengan conciencia (todav√≠a), pero s√≠ copian la idea de que muchas neuronas
    sencillas trabajando juntas pueden aprender cosas complejas.
</p>

<p>
    Eso permite que el deep aprenda patrones que ni nosotros podemos explicar bien:
    reconocer caras, traducir idiomas, detectar tumores, recomendarte qu√© ver en Netflix,
    escribir textos, decidir si en una foto hay o no hay algo.
</p>


</div>
""", unsafe_allow_html=True)
    
    st.image(
        "assets/images/vacasola.jpeg",
        caption="No todo en la vida es bueno, por eso:",
        use_container_width=True
    )

    st.markdown("""
<div class="card-explain">

<p><b>Peeero‚Ä¶ como todo lo bueno en esta vida, tiene su lado misterioso:  La famosa caja negra</b></p>

<p>
    Los modelos a veces aciertan de formas impresionantes y no siempre sabemos exactamente
    qu√© pas√≥ adentro. No es magia, es matem√°ticas‚Ä¶ pero matem√°ticas bien profundas.
</p>

<p>
    Eso hace que entender c√≥mo ‚Äúpiensan‚Äù sea un reto, pero tambi√©n lo vuelve un campo fascinante:
    es como estudiar el cerebro‚Ä¶ versi√≥n digital.
</p>


</div>
""", unsafe_allow_html=True)

    st.markdown('<div class="section-title">Ya con eso entendido, seguimos ¬øQu√© encontrar√°s a continuaci√≥n?</div>', unsafe_allow_html=True)
    st.markdown("""
    <ul class="custom-list">
        <li>Vas a ver resumenes acerca de lo qu√© es cada tema</li>
        <li>Tendremos v√≠deos recomendados, por si quieres saber m√°s y te llamo la atenci√≥n</li>
        <li>Enlaces a demos y herramientas interactivas en la web para que tu mismo lo modifiques</li>
        <li>Y si hay personas que siempre se preguntan y ¬øesto para que me sirve? Ac√° tambi√©n te damos la respuesta</li>
    </ul>
    """, unsafe_allow_html=True)

    st.markdown('<div class="section-title">Temas disponibles</div>', unsafe_allow_html=True)
    st.caption("Haz clic en la tarjeta que m√°s te llame la atenci√≥n y echa un primer vistazo a los temas del curso")

    # ========== TARJETAS DE TEMAS ========== #
    st.markdown('<div class="topic-grid">', unsafe_allow_html=True)

# Perceptr√≥n
    st.markdown(
    """
    <a href="?page=perceptron" class="topic-card card-perceptron">
        <div class="topic-header">
            <div class="topic-icon">‚ö°</div>
            <div class="topic-title-text">Perceptr√≥n</div>
        </div>
        <div class="topic-desc">
            La neurona artificial m√°s b√°sica: el inicio de las redes neuronales y la base
            para entender c√≥mo "aprende" una sola unidad.
        </div>
        <div class="topic-footer">
            Te interesa? Click ac√° ‚Üí
        </div>
    </a>
    """,
    unsafe_allow_html=True
    )

    st.markdown('</div>', unsafe_allow_html=True)

    # Transformers
    st.markdown(
        """
        <a href="?page=transformers" class="topic-card card-transformers">
            <div class="topic-header">
                <div class="topic-icon">üêØ</div>
                <div class="topic-title-text">Transformers</div>
            </div>
            <div class="topic-desc">
                No, no esos Transformers, ac√° no hay ning√∫n optimus prime, pero s√≠
                son los transformers que nos dicen c√≥mo es que los modelos
                entienden el texto completo usando atenci√≥n y les da vida a herramientas
                que todos utilizamos hoy d√≠a como ChatGPT.
            </div>
            <div class="topic-footer">
                ¬øQuieres ver este tema? Da click aqu√≠ ‚Üí
            </div>
        </a>
        """,
        unsafe_allow_html=True
    )

    # Callbacks
    st.markdown(
        """
        <a href="?page=callbacks" class="topic-card card-callbacks">
            <div class="topic-header">
                <div class="topic-icon">ü¶Å</div>
                <div class="topic-title-text">Callbacks</div>
            </div>
            <div class="topic-desc">
                Ac√° no vas a llamar a nadie #superar, en esta parte te hablaremos de 
                "trucos" para controlar el entrenamiento: parar a tiempo, guardar el mejor
                modelo, ajustar el learning rate, entre otros.
            </div>
            <div class="topic-footer">
                ¬øTe interesa? Click ac√° ‚Üí
            </div>
        </a>
        """,
        unsafe_allow_html=True
    )

    # Regularizaci√≥n
    st.markdown(
        """
        <a href="?page=regularizacion" class="topic-card card-regularizacion">
            <div class="topic-header">
                <div class="topic-icon">üõ°Ô∏è</div>
                <div class="topic-title-text">Regularizaci√≥n</div>
            </div>
            <div class="topic-desc">
                Aqu√≠ aprender√°s a ponerle ‚Äúfrenos‚Äù a tu red neuronal para que no memorice todo. 
                Veremos t√©cnicas como L1, L2, Dropout y Early Stopping para que tu modelo
                generalice mejor y sea m√°s confiable.
            </div>
            <div class="topic-footer">
                ¬øQuieres evitar el overfitting? Click ac√° ‚Üí
            </div>
        </a>
        """,
        unsafe_allow_html=True
    )


# LSTM
    st.markdown(
    """
    <a href="?page=LSTM" class="topic-card card-LSTM">
        <div class="topic-header">
            <div class="topic-icon">üåÄ</div>
            <div class="topic-title-text">LSTM</div>
        </div>
        <div class="topic-desc">
            No, no hablamos de tu memoria a corto plazo cuando olvidas d√≥nde dejaste las llaves. 
            Los LSTM son modelos que ‚Äîa diferencia de nosotros‚Äî s√≠ saben qu√© recordar y qu√© olvidar 
            mientras procesan texto paso a paso. Gracias a esa ‚Äúmemoria selectiva inteligente‚Äù, 
            dominaron el procesamiento de lenguaje antes de que los Transformers llegaran a robarse el show.
        </div>
        <div class="topic-footer">
            ¬øRecuerdas qu√© desayunaste? El LSTM s√≠. Explora aqu√≠ ‚Üí
        </div>
    </a>
    """,
    unsafe_allow_html=True
)


    # RNA
    st.markdown(
        """
        <a href="?page=rna" class="topic-card card-rna">
            <div class="topic-header">
                <div class="topic-icon">ü¶ä</div>
                <div class="topic-title-text">RNA</div>
            </div>
            <div class="topic-desc">
                Esta es la parte esencial, aqu√≠ se busca procesar datos de una manera
                similar a como lo hace el ser humano. O sea, donde se busca pensar
                como nosotros.
            </div>
            <div class="topic-footer">
                Yo s√© que quieres saber m√°s. Click ac√° ‚Üí
            </div>
        </a>
        """,
        unsafe_allow_html=True
    )

    # Embeddings
    st.markdown(
        """
        <a href="?page=embeddings" class="topic-card card-embedding">
            <div class="topic-header">
                <div class="topic-icon">üêñ</div>
                <div class="topic-title-text">Embedding</div>
            </div>
            <div class="topic-desc">
                No, no esas incrustaciones‚Ä¶  
                Porque s√≠, aunque en espa√±ol la 
                traducci√≥n t√©cnica sea literalmente ‚Äúincrustaciones‚Äù, 
                nadie en su sano juicio piensa en matem√°ticas cuando oye
                ‚Äúincrustar algo‚Äù.
            </div>
            <div class="topic-footer">
                ¬øIncrustar algo? Investiga, click aqu√≠ ‚Üí
            </div>
        </a>
        """,
        unsafe_allow_html=True
    )

    st.markdown('</div>', unsafe_allow_html=True)

    
    

    # LLM

    st.markdown(
    """
    <a href="?page=llm" class="topic-card card-transformers">
        <div class="topic-header">
            <div class="topic-icon">üß†</div>
            <div class="topic-title-text">LLM</div>
        </div>
        <div class="topic-desc">
            Los modelos de lenguaje gigantes que generan texto, entienden contexto y
            aprenden patrones del lenguaje a gran escala usando transformers.
        </div>
        <div class="topic-footer">
            Te interesa? Click ac√° ‚Üí
        </div>
    </a>
    """,
    unsafe_allow_html=True
    )

    st.markdown('</div>', unsafe_allow_html=True)


# ================== P√ÅGINA: TRANSFORMERS ================== #
elif page == "transformers":
    st.title("üêØ Transformers")
    st.markdown("---")
  
    st.subheader("Listo, ¬øde qu√© transformers estamos hablando?")
    st.markdown("""
Tranqui, ac√° no vamos a hablar de Optimus Prime ni de Bumblebee.  
Cuando decimos **Transformers** en Deep Learning, hablamos de una arquitectura de red neuronal
que se volvi√≥ la estrella del texto (y ahora tambi√©n de im√°genes, audio, etc.)

üí° La idea es: 

- Trabajan con secuencias (palabras, tokens, lo que sea).
- En vez de leer todo de izquierda a derecha sufriendo con la memoria (como las RNN),
- el modelo **ve la secuencia completa al tiempo** y decide a qu√© partes ponerle m√°s cuidado.

Ese ‚Äúponerle cuidado‚Äù es lo que se conoce como **auto-atenci√≥n** (*self-attention*).
    """)
    st.image(
        "assets/images/monopensando.jpeg"
    )

    st.subheader("¬øQu√© hace exactamente la auto-atenci√≥n?")
    st.markdown("""
Imagina esta frase:

> "La ni√±a dej√≥ el cuaderno en la mesa porque **estaba** roto."

T√∫, cerebro humano aparentemente funcional, sabes que ‚Äúestaba roto‚Äù se refiere al cuaderno,  
no a la mesa ni a la ni√±a.

La atenci√≥n hace algo parecido:

- cada palabra **mira** a todas las dem√°s
- decide cu√°les son m√°s importantes en ese contexto
- les asigna pesos (importancia)
- mezcla toda esa info
- y eso se repite por varias capas.

As√≠ el modelo no solo ve palabras sueltas, sino **relaciones** entre ellas.
Por eso a los Transformers se les da tan bien entender contexto.
    """)

    st.subheader("¬øPor qu√© los Transformers cambiaron el juego?")
    st.image(
        "assets/images/mapache alabando.jpeg"
    )
    st.markdown("""
Antes se usaban RNN y LSTM, que:

- le√≠an la secuencia en orden, una palabra detr√°s de otra,  
- se cansaban con secuencias largas,  
- y eran m√°s dif√≠ciles de paralelizar.

Con los Transformers:

- Se puede **paralelizar** el procesamiento ‚Üí entrenan mucho m√°s r√°pido.
- Capturan relaciones de largo alcance (cosas que pasan al inicio de un texto y afectan al final).
- Son la base de modelos como **GPT, BERT, T5, ViT** y en general la mayor√≠a de LLMs modernos.
- Escalan muy bien: m√°s datos + m√°s capas + m√°s par√°metros = modelos cada vez m√°s capaces (y tercos).
    """)

    st.subheader("¬øY aj√° ch√©vere y todo, pero yyy esto d√≥nde lo veo en la vida real?")
    st.markdown("""
Detr√°s de cosas como:

- ChatGPT y otros chatbots conversacionales.
- Traductores autom√°ticos que ya no suenan a Google Translator 2010.
- Motores de b√∫squeda m√°s inteligentes.
- Modelos que generan im√°genes y hasta c√≥digo.

Todo eso corre gracias a alg√∫n primo Transformer por ah√≠ trabajando 24/7 (Nadie esta siendo explotado ac√°).
    """)

    st.subheader("Si son curiosos y quieren entender m√°s, echenle un ojito a estos v√≠deos/Art√≠culos")
    st.write("Con uno o dos de estos ya agarras muy buena intuici√≥n:")

    col1, col2, col3= st.columns(3)
    with col1:
        st.video("https://www.youtube.com/watch?v=Kp4Mvapo5kc")  # Transformers explicado en espa√±ol
    with col2:
        st.video("https://www.youtube.com/watch?v=zxQyTK8quyY")  # Atenci√≥n y Transformers
    with col3:
        st.video("https://www.youtube.com/watch?v=eMlx5fFNoYc&t=1105s")
    
    st.markdown("""
-[¬øQu√© es un transformer?](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)
                Art√≠culo que explica que son los transformers
                """)
    st.subheader("Quieres verlo en acci√≥n por tu cuenta? Estas p√°ginas son perfectas para que juegues con par√°metros")
    st.markdown("""
- [Transformer Explainer (visual, interactivo)](https://poloclub.github.io/transformer-explainer/)  
  Puedes ver c√≥mo la atenci√≥n cambia palabra por palabra.
- [LLM 3D Visualizer](https://bbycroft.net/llm)  
  Una visualizaci√≥n 3D loqu√≠sima de c√≥mo se ve un modelo grande por dentro.
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)  
  Blog s√∫per visual, cero trauma con ecuaciones.
    """)

    st.subheader("Qu√© deber√≠as llevarte de este tema (lo mejor siempre para el final)")
    st.markdown("""
- Un Transformer **usa auto-atenci√≥n** para decidir qu√© partes del input son relevantes entre s√≠.  
- No recorre el texto ‚Äúa la antigua‚Äù, sino que **ve todo al tiempo**.  
- Funciona muy bien con texto, pero tambi√©n se ha adaptado a im√°genes, audio y m√°s.  
- Los **LLMs** (como ChatGPT) son b√°sicamente **Transformers gigantes** entrenados para predecir
  el siguiente token muchas, muchas, muchas veces.
    """)
    
    st.markdown("---")
    st.markdown('<div class="volver-btn">', unsafe_allow_html=True)
    if st.button("‚¨Ö Volver a Mundo Deep ‚≠ê"):
        st.experimental_set_query_params(page="home")
    st.markdown('</div>', unsafe_allow_html=True)



# ================== P√ÅGINA: CALLBACKS ================== #
elif page == "callbacks":
    st.title("üêû Callbacks")
    st.markdown("---")

    st.subheader("¬øQu√© es un callback en Deep Learning?")
    st.markdown("""
Piensa en un callback como ese amigo que te acompa√±a al gimnasio y te dice:

> ‚ÄúVamos, una m√°s, unita... no mentiras terminamos ah√≠‚Äù

o

> ‚ÄúP√°rele, sufiente d√≠a de pierna por hoy (van en la primera serie).‚Äù

Un *callback* hace **exactamente eso**, pero durante el entrenamiento de tu modelo.

Mientras el modelo entrena, el callback mira lo que est√° pasando y toma decisiones inteligentes:
- ‚ÄúOiga, usted si no cambia, ya no est√° mejorando ‚Üí pare.‚Äù
- ‚ÄúUy, esta √©poca qued√≥ mela caramela ‚Üí gu√°rdela.‚Äù
- ‚ÄúMmm‚Ä¶ est√° estancado, salga de ah√≠ ‚Üí b√°jele a la learning rate.‚Äù
- ‚ÄúQuiero ver esto en TensorBoard ‚Üí m√°ndeme logs.‚Äù

En pocas palabras:
**son asistentes que mejoran el entrenamiento sin que t√∫ metas mano cada rato.**
    """)

    st.subheader("¬øPero y por qu√© son √∫tiles?")
    st.markdown("""
Porque entrenar modelos puede ser as√≠:

- empieza s√∫per bien,  
- luego empieza a memorizar cosas que no deber√≠a,  
- luego se estanca,  
- luego empieza a improvisar como si fuera artista y alusina dur√≠simo.  

Los callbacks ayudan a:
- evitar el *overfitting*,  
- no perder el mejor modelo,  
- entrenar m√°s r√°pido,  
- ajustar par√°metros autom√°ticamente.  

Sin cambiar arquitectura, sin f√≥rmulas nuevas ‚Üí solo con un par de ‚Äúasistentes inteligentes‚Äù.
    """)

    st.subheader("Videitos que explican un poco m√°s")
    col1, col2 = st.columns(2)

    with col1:
        st.video("https://www.youtube.com/watch?v=lHkG0uZZ330")  # Explicaci√≥n simple de callbacks
    with col2:
        st.video("https://www.youtube.com/watch?v=N-1zpHn8xlI")  # EarlyStopping explicado

    st.caption("Ambos muestran los callbacks sin drama matem√°tico.")

    st.subheader("Y claramente si quieren explorar en estas p√°ginas pueden jugar con parametros")
    st.markdown("""
- ‚≠ê **Simula entrenamiento con EarlyStopping (interactivo):**  
  https://loss-landscape.playground.tensorflow.org/#callback=earlystop  

- ‚≠ê **Explora c√≥mo afecta la learning rate:**  
  https://poloclub.github.io/nnlosslandscape/  

- ‚≠ê **TensorBoard demo oficial:**  
  https://tensorboard.dev/  
    """)

    st.subheader("Los callbacks m√°s usados")
    st.markdown("""
**1. EarlyStopping**  
Como el amigo con el que uno esta y le dice "una y no m√°s" o "una y nos vamos" (Pero en este caso es verdad).

**2. ModelCheckpoint**  
El parcero que toma la foto siempre en el momento preciso.

**3. ReduceLROnPlateau**  
El parcero que dice que sean pareja al d√≠a de conocerse y uno le dice 
                "vamos m√°s lento".

**4. TensorBoard**  
Este es como el profesor que al final de semestre le va mostrando las notas de como va y el progreso
                (y uno reci√©n se va enterando que va perdiendo y que tiene que salvar el semestre en 1 semana)
    """)

    st.subheader("üí° Mini demo mental")
    st.markdown("""
Imagina entrenar por 50 √©pocas sin callbacks:

- tu modelo aprende bien y va melo 
- luego se empieza a pasar y sobreaprende y sigue.. y sigue y y y
- luego se da√±a...

Con callbacks, pasa as√≠:

- aprende bien (va melo) 
- EarlyStopping dice: ‚Äúepa, ya estuvo bien, no m√°s, deje ah√≠‚Äù  
- ModelCheckpoint dice: ‚Äúme quedo con la mejor versi√≥n, gracias a todos por participar.‚Äù  

Simple y efectivo.
    """)

    st.subheader("üîß C√≥digo para no olvidar")
    st.code("""
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

checkpoint = ModelCheckpoint(
    'mejor_modelo.keras',
    monitor='val_loss',
    save_best_only=True
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,
    batch_size=32,
    callbacks=[early_stop, checkpoint]
)
""", language="python")

    st.markdown("---")
    st.markdown('<div class="volver-btn">', unsafe_allow_html=True)
    if st.button("‚¨Ö Volver a Mundo Deep"):
        st.experimental_set_query_params(page="home")
    st.markdown('</div>', unsafe_allow_html=True)

############

elif page == "regularizacion":
    st.title("üõ°Ô∏è Regularizaci√≥n en Redes Neuronales")
    st.markdown("---")

    st.subheader("¬øQu√© es la regularizaci√≥n?")
    st.markdown("""
La **regularizaci√≥n** es un conjunto de t√©cnicas que ayudan a que una red neuronal **no se sobreentrene** con los datos de entrenamiento.  
El sobreentrenamiento (**overfitting**) ocurre cuando la red memoriza los datos en lugar de **aprender patrones generales**, lo que hace que falle con datos nuevos.

Piensa en la regularizaci√≥n como un **‚Äúfreno‚Äù** que evita que la red sea demasiado compleja.
    """)

    st.subheader("¬øPor qu√© es importante?")
    st.markdown("""
- Evita que la red aprenda ruido o detalles irrelevantes.  
- Mejora la **capacidad de generalizaci√≥n** a nuevos datos.  
- Hace que el modelo sea m√°s **estable y confiable**.
    """)

    st.subheader("Tipos principales de regularizaci√≥n")
    st.markdown("""
1. **L1 (Lasso)**  
   - Penaliza la suma de los valores absolutos de los pesos.  
   - Favorece que algunos pesos sean exactamente cero, generando un modelo m√°s **simple y escaso**.

2. **L2 (Ridge)**  
   - Penaliza la suma de los cuadrados de los pesos.  
   - Evita que los pesos tomen valores muy grandes, haciendo la red m√°s **suave y estable**.

3. **Dropout**  
   - Durante el entrenamiento, algunas neuronas se **apagan aleatoriamente** en cada iteraci√≥n.  
   - Esto fuerza a la red a no depender de neuronas espec√≠ficas, mejorando la **robustez**.

4. **Early Stopping**  
   - Se detiene el entrenamiento cuando el error en el conjunto de validaci√≥n **deja de mejorar**.  
   - Evita entrenar demasiado tiempo y sobreajustar los datos.
    """)

    st.subheader("Ejemplo de regularizaci√≥n en Keras")
    st.code("""
from tensorflow.keras import layers, models, regularizers

model = models.Sequential([
    layers.Dense(64, activation='relu', 
                 kernel_regularizer=regularizers.l2(0.01),
                 input_shape=(X_train.shape[1],)),
    layers.Dropout(0.3),
    layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Early stopping
from tensorflow.keras.callbacks import EarlyStopping
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = model.fit(X_train, y_train,
                    epochs=100,
                    batch_size=32,
                    validation_data=(X_valid, y_valid),
                    callbacks=[early_stop])
    """, language="python")

    st.subheader("Tips pr√°cticos")
    st.markdown("""
- Combina **L2 y Dropout** para obtener buena generalizaci√≥n.  
- Ajusta la **tasa de Dropout** entre 0.2 y 0.5 seg√∫n el tama√±o de tu red.  
- Monitorea siempre la **p√©rdida de validaci√≥n** para detectar sobreentrenamiento.  
- La regularizaci√≥n no reemplaza la necesidad de **buenos datos**: m√°s y mejores datos siempre ayudan.
    """)

    st.markdown("---")
    st.subheader("Video recomendado")
    col1, col2 = st.columns(2)
    with col1:
        st.video("https://www.youtube.com/watch?v=Q4Wc2zMYd2U")  # Explicaci√≥n r√°pida y clara
    with col2:
        st.video("https://www.youtube.com/watch?v=5tcbyHhsLJk")  # Columna vac√≠a si solo hay un video

    st.markdown("---")
    st.markdown('<div class="volver-btn">', unsafe_allow_html=True)
    if st.button("‚¨Ö Volver a Mundo Deep"):
        st.experimental_set_query_params(page="home")
    st.markdown('</div>', unsafe_allow_html=True)



    #######################RNA##########################3333
elif page == "rna":
    st.title("ü¶ä Redes Neuronales Artificiales (RNA)")
    st.markdown("---")

    st.subheader("¬øQu√© es una RNA?")
    st.markdown("""
Una **Red Neuronal Artificial (RNA)** es un modelo de inteligencia artificial inspirado en c√≥mo funciona
el cerebro humano. Su objetivo es **aprender patrones** a partir de datos y hacer predicciones o clasificaciones.

Piensa en ella como un conjunto de **neuronas artificiales** conectadas, que reciben informaci√≥n,
la procesan y generan una salida.
    """)

    st.subheader("¬øC√≥mo funciona una neurona artificial?")
    st.markdown("""
Cada neurona en la red realiza tres tareas principales:

1. **Recibe entradas**: valores num√©ricos que representan caracter√≠sticas de los datos (por ejemplo: edad, ingresos, temperatura).  
2. **Procesa la informaci√≥n**: multiplica cada entrada por un **peso** (qu√© tan importante es esa entrada), suma un **bias** y aplica una **funci√≥n de activaci√≥n** para decidir la salida.  
3. **Entrega un resultado**: que puede enviarse a otras neuronas en capas siguientes o como salida final.

El conjunto de estas neuronas conectadas permite **aprender relaciones complejas** que otros modelos simples no pueden.
    """)

    st.subheader("Funciones de activaci√≥n")
    st.markdown("""
Las **funciones de activaci√≥n** permiten que la red aprenda patrones complejos al introducir **no linealidad**:

- **Sigmoide**: valores entre 0 y 1, ideal para clasificaci√≥n binaria.  
- **ReLU (Rectified Linear Unit)**: devuelve 0 si el valor es negativo y el mismo valor si es positivo. Muy usada en capas ocultas.  
- **Tanh**: valores entre -1 y 1, ayuda a centrar los datos.
    """)

    st.subheader("Aprendizaje: c√≥mo la RNA mejora")
    st.markdown("""
La red **aprende ajustando pesos y bias** para que sus predicciones se acerquen a los resultados correctos:

- **Forward pass**: los datos pasan por la capa de entrada hasta generar la predicci√≥n.  
- **Backpropagation**: se calcula el error y se ajustan los pesos hacia atr√°s para reducirlo.  
- Este proceso se repite muchas veces hasta que la red domina los patrones.
    """)

    st.subheader("Usos pr√°cticos")
    st.markdown("""
Las RNA se usan en much√≠simas aplicaciones:

- üîç **Clasificaci√≥n**: riesgo crediticio, spam, diagn√≥stico m√©dico.  
- üìà **Regresi√≥n**: predicci√≥n de precios, ventas o demanda.  
- üñºÔ∏è **Visi√≥n por computadora**: reconocimiento facial, detecci√≥n de objetos.  
- üó£Ô∏è **Procesamiento de lenguaje**: chatbots, traducci√≥n autom√°tica, an√°lisis de sentimiento.  
- üéÆ **Aprendizaje por refuerzo**: agentes que aprenden a tomar decisiones √≥ptimas.
    """)

    st.subheader("Ejemplo visual en Keras")
    st.code("""
import tensorflow as tf
from tensorflow.keras import layers, models

model = models.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dense(32, activation='relu'),
    layers.Dense(1, activation='sigmoid')  # clasificaci√≥n binaria
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

history = model.fit(X_train, y_train,
                    epochs=50,
                    batch_size=32,
                    validation_data=(X_valid, y_valid))
    """, language="python")

    st.subheader("¬øD√≥nde vemos RNA en la vida real?")
    st.markdown("""
- Chatbots como ChatGPT  
- Reconocimiento facial en tu tel√©fono  
- Recomendaciones de productos en tiendas online  
- Predicciones m√©dicas y financieras
    """)

    st.markdown("---")
    st.subheader("Videos recomendados")
    col1, col2 = st.columns(2)
    with col1:
        st.video("https://www.youtube.com/watch?v=jKCQsndqEGQ")  # Buen video, bien explicado
    with col2:
        st.video("https://www.youtube.com/watch?v=Kovwua0Mmp4")  # Columna vac√≠a si solo hay un video

    st.markdown("---")
    st.markdown('<div class="volver-btn">', unsafe_allow_html=True)
    if st.button("‚¨Ö Volver a Mundo Deep"):
        st.experimental_set_query_params(page="home")
    st.markdown('</div>', unsafe_allow_html=True)

#12323hj3123h21nj3k12hbn2k3h2jk321hjk3

elif page == "regularizacion":
    st.title("üõ°Ô∏è Regularizaci√≥n en Redes Neuronales")
    st.markdown("---")

    st.subheader("¬øQu√© es la regularizaci√≥n?")
    st.markdown("""
La **regularizaci√≥n** es un conjunto de t√©cnicas que ayudan a que una red neuronal **no se sobreentrene** con los datos de entrenamiento.  
El sobreentrenamiento (**overfitting**) ocurre cuando la red memoriza los datos en lugar de **aprender patrones generales**, lo que hace que falle con datos nuevos.

Piensa en la regularizaci√≥n como un **‚Äúfreno‚Äù** que evita que la red sea demasiado compleja.
    """)

    st.subheader("¬øPor qu√© es importante?")
    st.markdown("""
- Evita que la red aprenda ruido o detalles irrelevantes.  
- Mejora la **capacidad de generalizaci√≥n** a nuevos datos.  
- Hace que el modelo sea m√°s **estable y confiable**.
    """)

    st.subheader("Tipos principales de regularizaci√≥n")
    st.markdown("""
1. **L1 (Lasso)**  
   - Penaliza la suma de los valores absolutos de los pesos.  
   - Favorece que algunos pesos sean exactamente cero, generando un modelo m√°s **simple y escaso**.

2. **L2 (Ridge)**  
   - Penaliza la suma de los cuadrados de los pesos.  
   - Evita que los pesos tomen valores muy grandes, haciendo la red m√°s **suave y estable**.

3. **Dropout**  
   - Durante el entrenamiento, algunas neuronas se **apagan aleatoriamente** en cada iteraci√≥n.  
   - Esto fuerza a la red a no depender de neuronas espec√≠ficas, mejorando la **robustez**.

4. **Early Stopping**  
   - Se detiene el entrenamiento cuando el error en el conjunto de validaci√≥n **deja de mejorar**.  
   - Evita entrenar demasiado tiempo y sobreajustar los datos.
    """)

    st.subheader("Ejemplo de regularizaci√≥n en Keras")
    st.code("""
from tensorflow.keras import layers, models, regularizers

model = models.Sequential([
    layers.Dense(64, activation='relu', 
                 kernel_regularizer=regularizers.l2(0.01),
                 input_shape=(X_train.shape[1],)),
    layers.Dropout(0.3),
    layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Early stopping
from tensorflow.keras.callbacks import EarlyStopping
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = model.fit(X_train, y_train,
                    epochs=100,
                    batch_size=32,
                    validation_data=(X_valid, y_valid),
                    callbacks=[early_stop])
    """, language="python")

    st.subheader("Tips pr√°cticos")
    st.markdown("""
- Combina **L2 y Dropout** para obtener buena generalizaci√≥n.  
- Ajusta la **tasa de Dropout** entre 0.2 y 0.5 seg√∫n el tama√±o de tu red.  
- Monitorea siempre la **p√©rdida de validaci√≥n** para detectar sobreentrenamiento.  
- La regularizaci√≥n no reemplaza la necesidad de **buenos datos**: m√°s y mejores datos siempre ayudan.
    """)

    st.markdown("---")
    st.subheader("Video recomendado")
    col1, col2 = st.columns(2)
    with col1:
        st.video("https://www.youtube.com/watch?v=Kovwua0Mmp4")  # Explicaci√≥n r√°pida y clara
    with col2:
        st.write("")  # Columna vac√≠a si solo hay un video

    st.markdown("---")
    st.markdown('<div class="volver-btn">', unsafe_allow_html=True)
    if st.button("‚¨Ö Volver a Mundo Deep"):
        st.experimental_set_query_params(page="home")
    st.markdown('</div>', unsafe_allow_html=True)



# ================== P√ÅGINA: Embbeddings ================== #

elif page == "embeddings":
    st.title("üêñ Embeddings")
    st.markdown("---")

    st.subheader("Antes que nada‚Ä¶ ¬øembeddings? ¬øIncrustaciones?")
    st.markdown("""
S√≠, en espa√±ol suena rar√≠simo: *‚Äúincrustaciones sem√°nticas‚Äù*.  
La primera vez que uno ve ese subt√≠tulo piensa que esto va de joyer√≠a y no de Deep Learning.

Pero no: un **embedding** es simplemente una forma inteligente de representar palabras como **vectores** 
que capturan *su significado*.  
Es el puente entre lenguaje humano ‚Üí matem√°ticas.
    """)

    st.subheader("¬øPor qu√© existen los embeddings?")
    st.markdown("""
Antes del boom de NLP moderno us√°bamos:

### One-Hot Encoding  
- Vectores gigantes llenos de ceros.  
- Cada palabra = un vector sin l√≥gica sem√°ntica.  
- ‚Äúperro‚Äù y ‚Äúgato‚Äù son tan distintos como ‚Äúperro‚Äù y ‚Äúwifi‚Äù.

### Bag-of-Words  
- Cuenta palabras pero ignora orden y significado.

Estos m√©todos no entienden **relaciones**.  
Ah√≠ es donde entran los embeddings.
    """)

    st.subheader("Embeddings: la idea corta")
    st.markdown("""
Los embeddings crean un **mapa sem√°ntico** donde:

- ‚Äúrey‚Äù y ‚Äúreina‚Äù quedan cerca.  
- ‚Äúperro‚Äù, ‚Äúperrito‚Äù, ‚Äúcanino‚Äù tambi√©n.  
- Las palabras raras o nuevas siguen teniendo representaci√≥n (FastText).  
- Cada vector captura relaciones reales del lenguaje.

Representan significado, no solo frecuencia.
    """)

    st.markdown("---")
    st.subheader("Tipos de embeddings (versi√≥n comprimida)")

    st.markdown("###1. **Embeddings No Contextuales** (vectores fijos por palabra)")
    st.markdown("""
Estos modelos producen **un solo vector por palabra**, sin importar la frase donde aparezca.

| Modelo | Tipo | Entrada | Objetivo |
|--------|------|---------|----------|
| **CBOW** | Predictivo | Palabras del contexto | Predecir la palabra central |
| **Skip-Gram** | Predictivo | Una palabra central | Predecir palabras del contexto |
| **FastText** | Predictivo con sub-palabras | n-gramas + palabra | Predecir contexto (mejor con palabras nuevas) |
| **GloVe** | Conteo + factorizaci√≥n | Coocurrencias globales | Factorizar matriz para obtener vectores |

> `"banco"` tendr√° el **mismo vector** en: ‚Äúme sent√© en el banco‚Äù y ‚Äúfui al banco‚Äù.
    """)

    st.markdown("###**Embeddings Contextuales** (vectores que cambian seg√∫n la frase)")
    st.markdown("""
Aqu√≠ la misma palabra tiene **vectores distintos** seg√∫n el contexto.

| Modelo | Red | Objetivo | ¬øContextual? |
|--------|-----|----------|--------------|
| **ELMo** | LSTM | Modelo de lenguaje | ‚úÖ |
| **BERT** | Transformer | Palabras enmascaradas | ‚úÖ |
| **GPT** | Transformer | Siguiente palabra | ‚úÖ |

> `"banco"` s√≠ cambia si hablas de dinero o de sentarte.
    """)

    st.markdown("---")
    st.subheader("CBOW vs Skip-Gram vs FastText (versi√≥n pr√°ctica)")

    st.markdown("""
| Tarea | Mejor m√©todo | Por qu√© |
|-------|--------------|----------|
| Conceptos similares | **CBOW** | Estable y r√°pido |
| Relaciones sem√°nticas complejas | **Skip-Gram** | Mejor en matices |
| Palabras nuevas / errores | **FastText** | Usa sub-palabras |
| Frases similares | **Skip-Gram** | Mayor recall |
    """)

    st.markdown("---")
    st.subheader("Embeddings m√°s usados en la vida real")
    st.markdown("""
- **Word2Vec (CBOW / Skip-Gram)** ‚Üí cl√°sico y r√°pido.  
- **FastText** ‚Üí entiende sub-palabras.  
- **GloVe** ‚Üí basado en coocurrencias globales.  
- **ELMo** ‚Üí contextual con LSTM.  
- **BERT embeddings** ‚Üí est√°ndar moderno.  
- **Sentence Transformers (SBERT)** ‚Üí para b√∫squedas, QA, similitud de frases.
    """)

    st.markdown("---")
    st.subheader("Videos recomendados")
    col1, col2 = st.columns(2)
    with col1:
        st.video("https://youtu.be/my5wFNQpFO0")  # Intuici√≥n
    with col2:
        st.video("https://youtu.be/hVM8qGRTaOA")  # Explicaci√≥n detallada

    st.markdown("---")
    st.subheader("Ahora probemos üß™ (Similitud del coseno con GloVe)")

    st.markdown("""
Aqu√≠ puedes cargar dos palabras y comparar sus **embeddings de GloVe**  
para ver qu√© tan parecidas son matem√°ticamente.
    """)

    # --- INTERACTIVE DEMO ---

    import os
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity

    @st.cache_resource
    def load_glove():
        embedding_index = {}
        glove_path = "glove.6B.50d.txt"
        if os.path.exists(glove_path):
            with open(glove_path, encoding="utf-8") as f:
                for line in f:
                    values = line.split()
                    word = values[0]
                    vector = np.asarray(values[1:], dtype="float32")
                    embedding_index[word] = vector
        return embedding_index

    embedding_index = load_glove()

    st.success(f"Embeddings cargados: {len(embedding_index)} palabras disponibles.")

    st.markdown("### Elige dos palabras para comparar:")
    colA, colB = st.columns(2)
    with colA:
        word1 = st.text_input("Primera palabra:", "king")
    with colB:
        word2 = st.text_input("Segunda palabra:", "queen")

    def get_vector(word):
        return embedding_index.get(word.lower(), None)

    if st.button("üîç Calcular similitud del coseno"):
        vec1 = get_vector(word1)
        vec2 = get_vector(word2)

        if vec1 is None:
            st.error(f"La palabra **{word1}** no existe en los embeddings.")
        elif vec2 is None:
            st.error(f"La palabra **{word2}** no existe en los embeddings.")
        else:
            similarity = cosine_similarity([vec1], [vec2])[0][0]

            st.markdown("### Resultados")
            st.write(f"**Vector de `{word1}`:**")
            st.code(vec1)

            st.write(f"**Vector de `{word2}`:**")
            st.code(vec2)

            st.metric(
                label=f"Similitud del coseno entre '{word1}' y '{word2}'",
                value=f"{similarity:.4f}"
            )

    st.markdown("---")
    st.markdown('<div class="volver-btn">', unsafe_allow_html=True)
    if st.button("‚¨Ö Volver a Mundo Deep"):
        st.experimental_set_query_params(page="home")
    st.markdown('</div>', unsafe_allow_html=True)

# ================== P√ÅGINA: LLMS ================== #
elif page == "llm":
    st.title("üß† LLMs")
    st.markdown("---")

    st.subheader("¬øQu√© es un LLM?")
    st.markdown("""
Un **LLM (Large Language Model)** es un modelo de deep learning entrenado para trabajar con lenguaje natural.
Su objetivo es simple de describir:

> ‚ÄúDado un texto, predice cu√°l deber√≠a ser el siguiente token.‚Äù

Esa tarea, repetida millones de veces con enormes cantidades de datos, permite que los LLMs:
- escriban textos coherentes,
- respondan preguntas,
- traduzcan,
- resuman,
- generen c√≥digo,
- mantengan conversaciones,
- y entiendan contexto de formas muy avanzadas.

Los LLMs funcionan gracias a una arquitectura: **los Transformers**, la base de casi todos los modelos modernos.
    """)

    st.subheader("¬øC√≥mo funciona un LLM por dentro?")
    st.markdown("""
1. **Tokenizaci√≥n**  
   El texto se divide en tokens (pedacitos optimizados, no palabras completas).

2. **Embeddings**  
   Cada token se convierte en un vector que representa su significado en un espacio matem√°tico.

3. **Capas de Transformer**  
   El modelo usa **auto-atenci√≥n** para encontrar relaciones entre tokens y entender contexto.

4. **Predicci√≥n autoregresiva**  
   El modelo predice el siguiente token, lo agrega a la secuencia‚Ä¶ y repite.

As√≠ generan texto, paso a paso.
    """)

    st.subheader("¬øQu√© hace que los LLMs funcionen tan bien?")
    st.markdown("""
- Se entrenan con cantidades enormes de datos.  
- Tienen millones o miles de millones de par√°metros.  
- Capturan patrones y estructuras del lenguaje.  
- Escalan incre√≠blemente bien: mientras m√°s grandes, mejor funcionan.
    """)

    st.subheader("V√≠deos recomendados")
    col1, col2 = st.columns(2)
    with col1:
        st.video("https://www.youtube.com/watch?v=MR7Dkyc7WSM")
    with col2:
        st.video("https://www.youtube.com/watch?v=wjZofJX0v4M&t=293s")

    st.subheader("Ejemplo en c√≥digo (Hugging Face Transformers)")
    st.code("""
from transformers import pipeline

# Crear un generador de texto con un modelo pre-entrenado
generator = pipeline("text-generation", model="gpt2")

salida = generator("Un LLM es un modelo que puede", max_length=40)
print(salida[0]['generated_text'])
    """, language="python")

    st.subheader("Intuici√≥n r√°pida")
    st.markdown("""
- Un LLM no ‚Äúpiensa‚Äù: **predice tokens**.  
- Aprende a partir de ejemplos, no reglas expl√≠citas.  
- Los Transformers son su cerebro principal.  
- ChatGPT, Gemini, Claude, Llama‚Ä¶ todos son LLMs gigantes.  
    """)

    st.markdown("""
¬øSab√≠as que un LLM nunca pierde en piedra, papel o tijera?

Porque **ya predijo** lo que ibas a sacar.  
*(Y si pierde dice que fue por ‚Äúoverfitting emocional‚Äù)* üòå
    """)

    st.subheader("Explora m√°s")
    st.markdown("""
Si quieres jugar con uno directamente (s√© que lo conoces):

üëâ **[Tenemos que hablar...](https://chat.openai.com)**

All√° puedes hacer lo mismo que aqu√≠, pero con m√°s poder, m√°s contexto y menos chistes de estad√≠stica  
(o m√°s, depende del d√≠a).
    """)

    st.markdown("---")
    st.markdown('<div class="volver-btn">', unsafe_allow_html=True)
    if st.button("‚¨Ö Volver a Mundo Deep"):
        st.experimental_set_query_params(page="home")
    st.markdown('</div>', unsafe_allow_html=True)

# ================== P√ÅGINA: PERCEPTRON ================== #
elif page == "perceptron":
    st.title("‚ö° Perceptr√≥n")
    st.markdown("---")

    st.subheader("¬øQu√© es un perceptr√≥n?")
    st.markdown("""
El **perceptr√≥n** es la unidad m√°s b√°sica de una red neuronal.  
En t√©rminos pr√°cticos:

> ‚ÄúEs una neurona artificial que toma entradas, las multiplica por pesos, suma un sesgo, aplica una funci√≥n de activaci√≥n y decide entre 0 y 1.‚Äù

Su meta principal es aprender a separar datos mediante una **l√≠nea recta** (o un plano si hay m√°s dimensiones).  
Eso s√≠: solo funciona si los datos son **linealmente separables**.
    """)

    st.subheader("¬øC√≥mo funciona?")
    st.markdown("""
Un perceptr√≥n sigue cuatro pasos:

1. **Multiplicaci√≥n**: cada entrada √ó su peso  
2. **Suma**: se agrega el sesgo (*bias*)  
3. **Activaci√≥n**: normalmente una funci√≥n escal√≥n  
4. **Predicci√≥n**: produce 0 o 1  

Es simple, pero es literalmente la base de TODAS las redes neuronales modernas.
    """)

    st.subheader("Ejemplo en c√≥digo (Perceptr√≥n cl√°sico)")
    st.code("""
import numpy as np

def activacion(x):
    return 1 if x >= 0 else 0

def entrenar(X, y, lr=0.1, epocas=10):
    pesos = np.zeros(X.shape[1])
    bias = 0

    for _ in range(epocas):
        for xi, yi in zip(X, y):
            salida = activacion(np.dot(xi, pesos) + bias)
            error = yi - salida
            pesos += lr * error * xi
            bias += lr * error
    return pesos, bias

X = np.array([[0,0],[0,1],[1,0],[1,1]])
y = np.array([0,1,1,1])  # OR

pesos, bias = entrenar(X, y)
print("Pesos:", pesos, "Bias:", bias)
    """, language="python")

    st.subheader("Chiste (porque sin chiste no cuenta üòå)")
    st.markdown("""
Un perceptr√≥n entra a un bar.  
El mesero le pregunta:

‚Äî ¬øVa a pedir algo?  
‚Äî Depende del *threshold*.  
*(Si no entendiste, no te preocupes: al perceptr√≥n a veces tampoco le da la activaci√≥n)* üòé
    """)
    st.subheader("Videos recomendados")
    col1, col2 = st.columns(2)
    with col1:
        st.video("https://www.youtube.com/watch?v=e9JYMng977Q")  # Perceptr√≥n b√°sico
    with col2:
        st.video("https://www.youtube.com/watch?v=pCy4yPzcCqs&t=77s&pp=ygUbcXXDqSBlcyB1biBwZXJjZXB0cm9uIGVuIG1s")  # OR, AND, XOR explicado visualmente

    st.subheader("Intuici√≥n r√°pida")
    st.markdown("""
- Es la unidad fundamental de una red neuronal  
- Aprende ajustando pesos y sesgos  
- Solo separa datos lineales  
- Es el precursor directo de las *Dense Layers* actuales  
    """)

    st.markdown("---")
    st.markdown('<div class="volver-btn">', unsafe_allow_html=True)
    if st.button("‚¨Ö Volver a Mundo Deep"):
        st.experimental_set_query_params(page="home")
    st.markdown('</div>', unsafe_allow_html=True)


# ================== P√ÅGINA: LSTM ================== #

elif page == "LSTM":
    st.title("üß† LSTM (Long Short-Term Memory)")
    st.markdown("---")

    st.subheader("Listo, ¬øqu√© es exactamente una LSTM?")
    st.markdown("""
No, no estamos hablando de tu memoria a corto plazo cuando olvidas d√≥nde dejaste las llaves.  
Las **LSTM** son redes que, a diferencia de nosotros, s√≠ saben qu√© recordar y qu√© olvidar  
mientras procesan texto paso a paso.  
Esa ‚Äúmemoria selectiva inteligente‚Äù las hizo famosas mucho antes del reinado de los Transformers.
    """)

    st.subheader("Entonces‚Ä¶ ¬øqu√© problema resolvieron las LSTM?")
    st.markdown("""
Antes de las LSTM ten√≠amos las **RNN cl√°sicas**, que intentaban recordar informaci√≥n secuencial.  
El problema era que se les **olvidaban cosas importantes** cuando la secuencia era larga  
o se confund√≠an con detalles irrelevantes.

Las LSTM introdujeron algo revolucionario:

**Un mecanismo interno capaz de decidir qu√© recordar, qu√© olvidar y qu√© usar.**

Esa es la verdadera innovaci√≥n.  
    """)

    st.subheader("La idea humana detr√°s de las compuertas de una LSTM")
    st.markdown("""
Piensa en una LSTM como un amigo que escucha tu historia y tiene una memoria selectiva muy fina:
solo guarda lo que importa y descarta lo irrelevante.

Para lograrlo, usa **tres compuertas** y un estado interno:

---

### üö™1. *Compuerta de olvido* ‚Äî ‚Äú¬øQu√© parte del pasado vale la pena guardar?‚Äù
Decide qu√© fracci√≥n de lo que ven√≠a antes **se conserva (‚âà1)**  
y qu√© fracci√≥n **se elimina (‚âà0)**.

Es la que permite que la red **no se quede pegada a informaci√≥n obsoleta**.

---

### üìù2. *Compuerta de entrada + candidatos* ‚Äî ‚Äú¬øQu√© nueva info deber√≠a a√±adir?‚Äù
- La compuerta de entrada decide **d√≥nde** escribir.  
- Los candidatos proponen **qu√©** podr√≠amos almacenar si vale la pena.

Esto evita que la red agregue ruido o datos innecesarios.

---

### üß±3. *Actualizaci√≥n del estado de la celda* ‚Äî ‚ÄúConstruyamos la nueva memoria‚Äù
Aqu√≠ se mezcla:
- lo que sobrevivi√≥ del pasado (olvido),  
- con lo que se aprob√≥ del presente (entrada).

Es la **memoria larga** de la LSTM.

---

### üîé4. *Compuerta de salida* ‚Äî ‚Äú¬øQu√© parte de la memoria mostramos al exterior?‚Äù
Decide qu√© parte del estado interno se expone.  
Luego aplica una especie de filtro suave (tanh) para que no se salga de control.

Ese resultado es lo que se pasa a la siguiente capa/tiempo.

---

En resumen:

> Las compuertas son filtros inteligentes que mantienen la memoria √∫til,  
> descartan basura y permiten usar la informaci√≥n correcta en el momento adecuado.
    """)

    st.subheader("¬øY en qu√© mejora esto frente a otros modelos?")
    st.markdown("""
Comparado con modelos previos, una LSTM:

- **supera el olvido r√°pido** de las RNN tradicionales,  
- maneja secuencias m√°s largas,  
- tiene memoria de largo y corto plazo,  
- evita que los gradientes se destruyan (el famoso *vanishing gradient*),  
- y funciona muy bien en tareas donde **el orden importa**.

### üÜö Comparaci√≥n r√°pida

- **RNN cl√°sica:** recuerda poco, se confunde r√°pido.  
- **LSTM:** recuerda lo importante con filtros inteligentes.  
- **GRU:** versi√≥n simplificada, m√°s r√°pida y casi igual de buena.  
- **Transformers:** ven todo al tiempo, sin memoria recurrente.

Aun as√≠, las LSTM siguen siendo muy √∫tiles cuando quieres modelos peque√±os, ligeros o cuando la secuencia **s√≠ depende del paso anterior**.
    """)

    st.subheader("¬øD√≥nde se usan en la vida real?")
    st.markdown("""
- Modelos de texto tradicionales  
- Predicci√≥n de series temporales  
- Clasificaci√≥n de sentimiento  
- Procesamiento secuencial donde importa el orden  
- Aplicaciones en voz y audio

Antes del boom de Transformers, las LSTM eran las reinas absolutas del NLP.
    """)

    st.subheader("Videos recomendados (modo visual activado)")
    st.write("Con estos dos videos entiendes la intuici√≥n y el funcionamiento interno:")

    col1, col2 = st.columns(2)
    with col1:
        st.video("https://youtu.be/1BubAvTVBYs")  # Intuici√≥n LSTM
    with col2:
        st.video("https://youtu.be/8HyCNIVRbSU")  # Explicaci√≥n completa LSTM/GRU

    st.subheader("Qu√© deber√≠as llevarte de este tema (versi√≥n resumen)")
    st.markdown("""
- Una LSTM decide **qu√© recordar, qu√© olvidar y qu√© usar** mediante compuertas inteligentes.  
- Fue creada para resolver el problema de **memoria corta** de las RNN cl√°sicas.  
- Funciona muy bien con secuencias largas.  
- Las **GRU** son hermanas simplificadas de las LSTM.  
- Aunque los Transformers dominan hoy, las LSTM siguen siendo muy √∫tiles en muchos escenarios.
    """)

    st.markdown("---")
    st.markdown('<div class="volver-btn">', unsafe_allow_html=True)
    if st.button("‚¨Ö Volver a Mundo Deep"):
        st.experimental_set_query_params(page="home")

    st.markdown('</div>', unsafe_allow_html=True)
